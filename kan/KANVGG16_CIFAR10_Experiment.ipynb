{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSQpS0W8jMoN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# KANLinear class definition as provided\n",
        "class KANLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=5, spline_order=3, scale_noise=0.1, scale_base=1.0, scale_spline=1.0, enable_standalone_scale_spline=True, base_activation=nn.SiLU, grid_eps=0.02, grid_range=[-1, 1]):\n",
        "        super(KANLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = ((torch.arange(-spline_order, grid_size + spline_order + 1) * h + grid_range[0]).expand(in_features, -1).contiguous())\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size + spline_order))\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = ((torch.rand(self.grid_size + 1, self.in_features, self.out_features) - 1 / 2) * self.scale_noise / self.grid_size)\n",
        "            self.spline_weight.data.copy_((self.scale_spline if not self.enable_standalone_scale_spline else 1.0) * self.curve2coeff(self.grid.T[self.spline_order : -self.spline_order], noise))\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        grid = self.grid\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = ((x - grid[:, : -(k + 1)]) / (grid[:, k:-1] - grid[:, : -(k + 1)]) * bases[:, :, :-1]) + ((grid[:, k + 1 :] - x) / (grid[:, k + 1 :] - grid[:, 1:(-k)]) * bases[:, :, 1:])\n",
        "        assert bases.size() == (x.size(0), self.in_features, self.grid_size + self.spline_order)\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "        A = self.b_splines(x).transpose(0, 1)\n",
        "        B = y.transpose(0, 1)\n",
        "        solution = torch.linalg.lstsq(A, B).solution\n",
        "        result = solution.permute(2, 0, 1)\n",
        "        assert result.size() == (self.out_features, self.in_features, self.grid_size + self.spline_order)\n",
        "        return result.contiguous()\n",
        "\n",
        "    @property\n",
        "    def scaled_spline_weight(self):\n",
        "        return self.spline_weight * (self.spline_scaler.unsqueeze(-1) if self.enable_standalone_scale_spline else 1.0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(self.b_splines(x).view(x.size(0), -1), self.scaled_spline_weight.view(self.out_features, -1))\n",
        "        return base_output + spline_output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        batch = x.size(0)\n",
        "        splines = self.b_splines(x).permute(1, 0, 2)\n",
        "        orig_coeff = self.scaled_spline_weight.permute(1, 2, 0)\n",
        "        unreduced_spline_output = torch.bmm(splines, orig_coeff).permute(1, 0, 2)\n",
        "        x_sorted = torch.sort(x, dim=0)[0]\n",
        "        grid_adaptive = x_sorted[torch.linspace(0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device)]\n",
        "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
        "        grid_uniform = (torch.arange(self.grid_size + 1, dtype=torch.float32, device=x.device).unsqueeze(1) * uniform_step + x_sorted[0] - margin)\n",
        "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
        "        grid = torch.cat([grid[:1] - uniform_step * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1), grid, grid[-1:] + uniform_step * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1)], dim=0)\n",
        "        self.grid.copy_(grid.T)\n",
        "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        l1_fake = self.spline_weight.abs().mean(-1)\n",
        "        regularization_loss_activation = l1_fake.sum()\n",
        "        p = l1_fake / regularization_loss_activation\n",
        "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
        "        return regularize_activation * regularization_loss_activation + regularize_entropy * regularization_loss_entropy\n",
        "\n"
      ],
      "metadata": {
        "id": "oKAls8iErpvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kode versi terbaru ini ditambahkan batch_normalisasi pada setiap akhir layer conv\n",
        "\n",
        "1. Stabilisasi Distribusi Aktivasi:\n",
        "\n",
        "Batch Normalization menstabilkan distribusi aktivasi jaringan dengan menormalkan output dari setiap layer berdasarkan mini-batch.\n",
        "Ini membantu mengurangi masalah \"vanishing gradients\" (gradien menghilang) dan \"exploding gradients\" (gradien meledak), yang sering menjadi kendala dalam pelatihan jaringan yang dalam.\n",
        "\n",
        "2. Akselerasi Konvergensi:\n",
        "\n",
        "Dengan menormalkan input di setiap lapisan, model dapat menggunakan tingkat pembelajaran yang lebih tinggi tanpa takut terjebak dalam lokal minima atau divergen.\n",
        "Hal ini mempercepat proses pelatihan karena model lebih cepat menemukan optimal point.\n",
        "\n",
        "3. Regularisasi Implisit:\n",
        "\n",
        "Batch Normalization menambahkan sedikit noise pada setiap mini-batch selama pelatihan, yang bertindak sebagai regularizer dan membantu mencegah overfitting.\n",
        "Ini membuat model lebih generalizable ke data yang belum pernah dilihat sebelumnya.\n",
        "\n",
        "4. Mengurangi Ketergantungan pada Inisialisasi Parameter:\n",
        "\n",
        "Model menjadi lebih robust terhadap inisialisasi parameter yang buruk karena Batch Normalization mengontrol skala aktivasi.\n",
        "Ini memungkinkan penggunaan berbagai skema inisialisasi tanpa kehilangan performa.\n",
        "\n",
        "5. Mengurangi Sensitivitas terhadap Skala Input:\n",
        "\n",
        "Dengan menormalkan data dalam mini-batch, Batch Normalization mengurangi sensitivitas model terhadap skala dan distribusi input awal.\n",
        "Ini membantu dalam situasi di mana data memiliki berbagai macam skala dan distribusi."
      ],
      "metadata": {
        "id": "SQjL3AgZ6LXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KANVGG16(nn.Module):\n",
        "    def __init__(self, num_classes=10):  # CIFAR100 has 10 classes\n",
        "        super(KANVGG16, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Convolutional block 1\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            # Convolutional block 2\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "            # Convolutional block 3\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "\n",
        "            # Convolutional block 4\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            # Convolutional block 5\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            KANLinear(512 * 1 * 1, 4096),  # Adjusted for CIFAR100 image size\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            KANLinear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            KANLinear(4096, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "HLtvWZ3csBFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Instantiate the model, define the loss function and the optimizer\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = KANVGG16(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_loss = float('inf')\n",
        "trigger_times = 0\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Split train set into train and validation sets\n",
        "train_indices, val_indices = train_test_split(list(range(len(trainset))), test_size=0.2, stratify=trainset.targets)\n",
        "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler, num_workers=2)\n",
        "valloader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=val_sampler, num_workers=2)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(50):  # Maximum epochs\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    train_losses.append(running_loss / len(trainloader))\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_losses.append(val_loss / len(valloader))\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {train_losses[-1]:.3f}, Validation Loss: {val_losses[-1]:.3f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Check early stopping condition\n",
        "    if val_losses[-1] < best_loss:\n",
        "        best_loss = val_losses[-1]\n",
        "        trigger_times = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print('Early stopping!')\n",
        "            break\n",
        "\n",
        "print(\"Finished Training\")\n",
        "\n",
        "# Plot training and validation loss\n",
        "def plot_loss(train_losses, val_losses):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot validation accuracy\n",
        "def plot_accuracy(val_accuracies):\n",
        "    plt.figure()\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the losses and accuracy\n",
        "plot_loss(train_losses, val_losses)\n",
        "plot_accuracy(val_accuracies)\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%\")\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QoiuUsG1s6iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Input jumlah epoch\n",
        "jumlah_epoch = 2\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize model\n",
        "model = KANVGG16(num_classes=10).to(device)\n",
        "print(model)\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# DataParallel if more than one GPU is available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Function to print model parameter details\n",
        "def print_parameter_details(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.size()} {'requires_grad' if param.requires_grad else 'frozen'}\")\n",
        "\n",
        "print_parameter_details(model)\n",
        "\n",
        "# Define transformations for training and validation data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
        "\n",
        "# Split training set into train and validation sets\n",
        "train_indices, val_indices = train_test_split(list(range(len(trainset))), test_size=0.2, stratify=trainset.targets)\n",
        "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "# Define DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=val_sampler, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_loader), epochs=100)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(inputs)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return running_loss / len(train_loader), accuracy\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Validation Epoch: {epoch} [{batch_idx * len(inputs)}/{len(val_loader.dataset)} ({100. * batch_idx / len(val_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(classification_report(all_labels, all_predictions))\n",
        "    return val_loss / len(val_loader), accuracy\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "# Lists to store loss and accuracy values\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Timing variables\n",
        "epoch_times = []\n",
        "total_start_time = time.time()\n",
        "\n",
        "# Function to convert seconds to hours, minutes, seconds\n",
        "def convert_seconds(seconds):\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    seconds = int(seconds % 60)\n",
        "    return hours, minutes, seconds\n",
        "\n",
        "for epoch in range(jumlah_epoch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "    val_loss, val_accuracy = validate(model, val_loader, criterion, device, epoch)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{jumlah_epoch}, Train Loss: {train_loss:.6f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_duration)\n",
        "    hours, minutes, seconds = convert_seconds(epoch_duration)\n",
        "    print(f\"Epoch {epoch+1} duration: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model_weights_KAN.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "total_hours, total_minutes, total_seconds = convert_seconds(total_duration)\n",
        "print(f\"Total training time: {total_hours} hours, {total_minutes} minutes, {total_seconds} seconds\")\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_weights_KAN.pth'))\n",
        "\n",
        "# Evaluate model on test set\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy, np.array(all_predictions), np.array(all_labels)\n",
        "\n",
        "accuracy, preds, true_labels = test(model, test_loader, device)\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Save predictions and labels for ROC plotting\n",
        "np.save('preds.npy', preds)\n",
        "np.save('true_labels.npy', true_labels)\n",
        "\n",
        "# Plot training and validation loss\n",
        "def plot_loss(train_losses, val_losses):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "def plot_accuracy(train_accuracies, val_accuracies):\n",
        "    plt.figure()\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(train_losses, val_losses)\n",
        "plot_accuracy(train_accuracies, val_accuracies)\n",
        "\n",
        "for i, epoch_time in enumerate(epoch_times):\n",
        "    hours, minutes, seconds = convert_seconds(epoch_time)\n",
        "    print(f\"Epoch {i+1} duration: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
        ""
      ],
      "metadata": {
        "id": "LQiM8CEK7Qb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load predictions and true labels\n",
        "preds = np.load('preds.npy')\n",
        "true_labels = np.load('true_labels.npy')\n",
        "\n",
        "# Function to plot ROC curve for multi-class classification\n",
        "def plot_roc_curve(true_labels, preds, n_classes):\n",
        "    true_labels_bin = label_binarize(true_labels, classes=np.arange(n_classes))\n",
        "\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(true_labels_bin[:, i], preds[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    plt.figure()\n",
        "    colors = plt.cm.get_cmap('tab10').colors\n",
        "    for i in range(n_classes):\n",
        "        plt.plot(fpr[i], tpr[i], color=colors[i], lw=2,\n",
        "                 label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic for CIFAR-10')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Function to calculate and print classification metrics for multi-class classification\n",
        "def print_classification_metrics(true_labels, preds, n_classes):\n",
        "    preds_labels = np.argmax(preds, axis=1)\n",
        "    accuracy = accuracy_score(true_labels, preds_labels)\n",
        "    precision = precision_score(true_labels, preds_labels, average='macro')\n",
        "    recall = recall_score(true_labels, preds_labels, average='macro')\n",
        "    f1 = f1_score(true_labels, preds_labels, average='macro')\n",
        "    report = classification_report(true_labels, preds_labels, target_names=[f'Class {i}' for i in range(n_classes)])\n",
        "\n",
        "    print(\" \")\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "    print('Classification Report:')\n",
        "    print(report)\n",
        "\n",
        "# Plot ROC curve and print classification metrics\n",
        "n_classes = 10\n",
        "plot_roc_curve(true_labels, preds, n_classes)\n",
        "print_classification_metrics(true_labels, preds, n_classes)"
      ],
      "metadata": {
        "id": "K23LJuKx3jAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "epoch = 2\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = KANVGG16(num_classes=10).to(device)\n",
        "\n",
        "# Print model summary\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "\n",
        "# Use DataParallel if multiple GPUs are available\n",
        "if torch.cuda.device_count() > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "# Define transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_val = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.CenterCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_val)\n",
        "\n",
        "train_indices, val_indices = train_test_split(list(range(len(trainset))), test_size=0.2, stratify=trainset.targets)\n",
        "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=train_sampler, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(trainset, batch_size=64, sampler=val_sampler, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=len(train_loader), epochs=epoch)\n",
        "\n",
        "# Function to train the model\n",
        "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(inputs)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return running_loss / len(train_loader), accuracy\n",
        "\n",
        "# Function to validate the model\n",
        "def validate(model, val_loader, criterion, device, epoch):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(val_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Validation Epoch: {epoch} [{batch_idx * len(inputs)}/{len(val_loader.dataset)} ({100. * batch_idx / len(val_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(classification_report(all_labels, all_predictions))\n",
        "    return val_loss / len(val_loader), accuracy\n",
        "\n",
        "# Early stopping parameters\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "# Lists to store loss and accuracy values\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Timing variables\n",
        "epoch_times = []\n",
        "total_start_time = time.time()\n",
        "\n",
        "# Function to convert seconds to hours, minutes, seconds\n",
        "def convert_seconds(seconds):\n",
        "    hours = int(seconds // 3600)\n",
        "    minutes = int((seconds % 3600) // 60)\n",
        "    seconds = int(seconds % 60)\n",
        "    return hours, minutes, seconds\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epoch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "    val_loss, val_accuracy = validate(model, val_loader, criterion, device, epoch)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/100, Train Loss: {train_loss:.6f}, Train Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    epoch_duration = epoch_end_time - epoch_start_time\n",
        "    epoch_times.append(epoch_duration)\n",
        "    hours, minutes, seconds = convert_seconds(epoch_duration)\n",
        "    print(f\"Epoch {epoch+1} duration: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model_weights_KAN.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "total_end_time = time.time()\n",
        "total_duration = total_end_time - total_start_time\n",
        "total_hours, total_minutes, total_seconds = convert_seconds(total_duration)\n",
        "print(f\"Total training time: {total_hours} hours, {total_minutes} minutes, {total_seconds} seconds\")\n",
        "\n",
        "model.load_state_dict(torch.load('best_model_weights_KAN.pth'))\n",
        "\n",
        "# Evaluate model on test set and save predictions and true labels\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probs = torch.softmax(outputs, dim=1)  # Use softmax to get class probabilities\n",
        "            _, predicted = torch.max(probs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_preds.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    np.save('probs.npy', all_preds)  # Save probabilities instead of class indices\n",
        "    np.save('labels.npy', all_labels)\n",
        "    return accuracy\n",
        "\n",
        "accuracy = test(model, test_loader, device)\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Plot training and validation loss\n",
        "def plot_loss(train_losses, val_losses):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "def plot_accuracy(train_accuracies, val_accuracies):\n",
        "    plt.figure()\n",
        "    plt.plot(train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_loss(train_losses, val_losses)\n",
        "plot_accuracy(train_accuracies, val_accuracies)\n",
        "\n",
        "for i, epoch_time in enumerate(epoch_times):\n",
        "    hours, minutes, seconds = convert_seconds(epoch_time)\n",
        "    print(f\"Epoch {i+1} duration: {hours} hours, {minutes} minutes, {seconds} seconds\")\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "Io33NJEB-wRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from scipy import interp\n",
        "\n",
        "# Load predictions and labels\n",
        "y_pred = np.load('probs.npy')\n",
        "y_true = np.load('labels.npy')\n",
        "\n",
        "# Binarize the labels for multi-class ROC computation\n",
        "y_true_bin = label_binarize(y_true, classes=np.arange(10))\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(10):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Compute macro-average ROC curve and ROC area\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(10)]))\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(10):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "mean_tpr /= 10\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='Micro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"micro\"]),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='Macro-average ROC curve (area = {0:0.2f})'\n",
        "               ''.format(roc_auc[\"macro\"]),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(10), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                   ''.format(i, roc_auc[i]))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "twnlYcMs_Yc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}