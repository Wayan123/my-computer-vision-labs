{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX8mhOLljYeM"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <a href=\"url\"><img src=\"https://www.americanhumane.org/app/uploads/2016/08/shutterstock_162633491.jpg\" align=\"middle\" height=\"383\" width=\"640\" ></a>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPThEjGg0jUw"
      },
      "source": [
        "### Load dataset from gdrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vnuLcT_dGoxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 000-dataset-catdog-25000\n",
        "\n",
        "!ls '/content/drive/MyDrive/000-dataset-catdog-25000/valid'"
      ],
      "metadata": {
        "id": "X395tgawG-eK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download dataset validation"
      ],
      "metadata": {
        "id": "8cgvS1u4ONMU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "Isi dari .zip di extract ke directory `/tmp/train`, dimana setiap folder berisi subfolder dengan nama `dandelion` dan `rumput`.\n",
        "\n",
        "Kita menggunakan [ImageGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) class untuk membuat secara otomatis dataset train dari directory ini dengan menggunakan subdirectory untuk setiap kelasnya.\n",
        "\n",
        "Menentukan setiap subdirectory :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NR_M9nWN-K8B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cat_dir = os.path.join('/content/drive/MyDrive/000-dataset-catdog-25000/train/cat')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dog_dir = os.path.join('/content/drive/MyDrive/000-dataset-catdog-25000/train/dog')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "valid_cat_dir = os.path.join('/content/drive/MyDrive/000-dataset-catdog-25000/valid/cat')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "valid_dog_dir = os.path.join('/content/drive/MyDrive/000-dataset-catdog-25000/valid/dog')\n",
        "\n",
        "# Directory with our test cat and dog pictures\n",
        "test_cat_and_dog_dir = os.path.join('/content/drive/MyDrive/000-dataset-catdog-25000/test1/test1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBYtA_Zd8_T"
      },
      "source": [
        "Melihat seperti apa nama file di training directory `dandelion` dan `rumput` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PIP1rkmeAYS"
      },
      "outputs": [],
      "source": [
        "train_cat_names = os.listdir(train_cat_dir)\n",
        "print(train_cat_names[:10])\n",
        "\n",
        "train_dog_names = os.listdir(train_dog_dir)\n",
        "print(train_dog_names[:10])\n",
        "\n",
        "validation_cat_names = os.listdir(valid_cat_dir)\n",
        "print(validation_cat_names[:10])\n",
        "\n",
        "validation_dog_names = os.listdir(valid_dog_dir)\n",
        "print(validation_dog_names[:10])\n",
        "\n",
        "testing_cat_and_dog_names = os.listdir(test_cat_and_dog_dir)\n",
        "print(testing_cat_and_dog_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlqN5KbafhLI"
      },
      "source": [
        "Mencari informasi berapa total images dari dandelion dan rumput di directory :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4XHh2xSfgie"
      },
      "outputs": [],
      "source": [
        "print('total training cat images:', len(os.listdir(train_cat_dir)))\n",
        "print('total training dog images:', len(os.listdir(train_dog_dir)))\n",
        "print('total validation cat images:', len(os.listdir(valid_cat_dir)))\n",
        "print('total validation dog images:', len(os.listdir(valid_dog_dir)))\n",
        "print('total testing cat and dog images:', len(os.listdir(test_cat_and_dog_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3WZABE9eX-8"
      },
      "source": [
        "Melihat beberapa pictures untuk mendapatkan sense lebih baik Seperti apa.\n",
        "Hal pertama yang dilakukan adalah melakukan konfigurasi matplot parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2_Q0-_5UAv-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# Index for iterating over images\n",
        "pic_index = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvHzGCxXkqp"
      },
      "source": [
        "Menampilkan 8 batch dari 8 dandelion and 8 rumput pictures. Kamu bisa melakukan rerun cell kembali untuk melihat fresh batch setiap waktu:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wpr8GxjOU8in"
      },
      "outputs": [],
      "source": [
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_cat_pic = [os.path.join(train_cat_dir, fname)\n",
        "                for fname in train_cat_names[pic_index-8:pic_index]]\n",
        "next_dog_pic = [os.path.join(train_dog_dir, fname)\n",
        "                for fname in train_dog_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_cat_pic + next_dog_pic):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "#### Data Preprocessing\n",
        "\n",
        "Menggunakan `keras.preprocessing.image.ImageDataGenerator` class untuk membuat  train and validation dataset dan normalize data.\n",
        "Sangat penting untuk melakukan normalize data karena data akan diproses oleh CNN untuk improve performance secara keseluruhan. Disini kita akan menggunakan `rescale` parameter untuk scale our image pixel values dari [0, 255] ke [0,1].\n",
        "\n",
        "Disetiap generator, kita menentukan source directory dari images, classes, input image size, batch size (seberapa banyak images yang akan diproses), dan class mode."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "qwJvez2zRO8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Definisi KANLinear\n",
        "  - Definisi ini mengikuti sumber yang diberikan dan digunakan dalam model CNNKAN."
      ],
      "metadata": {
        "id": "Y8sTGvYCjt6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj0_6IszQeIF"
      },
      "outputs": [],
      "source": [
        "class KANLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=5, spline_order=3, scale_noise=0.1, scale_base=1.0, scale_spline=1.0, enable_standalone_scale_spline=True, base_activation=nn.SiLU, grid_eps=0.02, grid_range=[-1, 1]):\n",
        "        super(KANLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = ((torch.arange(-spline_order, grid_size + spline_order + 1) * h + grid_range[0]).expand(in_features, -1).contiguous())\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size + spline_order))\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = ((torch.rand(self.grid_size + 1, self.in_features, self.out_features) - 1 / 2) * self.scale_noise / self.grid_size)\n",
        "            self.spline_weight.data.copy_((self.scale_spline if not self.enable_standalone_scale_spline else 1.0) * self.curve2coeff(self.grid.T[self.spline_order : -self.spline_order], noise))\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        grid = self.grid\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = ((x - grid[:, : -(k + 1)]) / (grid[:, k:-1] - grid[:, : -(k + 1)]) * bases[:, :, :-1]) + ((grid[:, k + 1 :] - x) / (grid[:, k + 1 :] - grid[:, 1:(-k)]) * bases[:, :, 1:])\n",
        "        assert bases.size() == (x.size(0), self.in_features, self.grid_size + self.spline_order)\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "        A = self.b_splines(x).transpose(0, 1)\n",
        "        B = y.transpose(0, 1)\n",
        "        solution = torch.linalg.lstsq(A, B).solution\n",
        "        result = solution.permute(2, 0, 1)\n",
        "        assert result.size() == (self.out_features, self.in_features, self.grid_size + self.spline_order)\n",
        "        return result.contiguous()\n",
        "\n",
        "    @property\n",
        "    def scaled_spline_weight(self):\n",
        "        return self.spline_weight * (self.spline_scaler.unsqueeze(-1) if self.enable_standalone_scale_spline else 1.0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(self.b_splines(x).view(x.size(0), -1), self.scaled_spline_weight.view(self.out_features, -1))\n",
        "        return base_output + spline_output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        batch = x.size(0)\n",
        "        splines = self.b_splines(x).permute(1, 0, 2)\n",
        "        orig_coeff = self.scaled_spline_weight.permute(1, 2, 0)\n",
        "        unreduced_spline_output = torch.bmm(splines, orig_coeff).permute(1, 0, 2)\n",
        "        x_sorted = torch.sort(x, dim=0)[0]\n",
        "        grid_adaptive = x_sorted[torch.linspace(0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device)]\n",
        "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
        "        grid_uniform = (torch.arange(self.grid_size + 1, dtype=torch.float32, device=x.device).unsqueeze(1) * uniform_step + x_sorted[0] - margin)\n",
        "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
        "        grid = torch.cat([grid[:1] - uniform_step * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1), grid, grid[-1:] + uniform_step * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1)], dim=0)\n",
        "        self.grid.copy_(grid.T)\n",
        "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        l1_fake = self.spline_weight.abs().mean(-1)\n",
        "        regularization_loss_activation = l1_fake.sum()\n",
        "        p = l1_fake / regularization_loss_activation\n",
        "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
        "        return regularize_activation * regularization_loss_activation + regularize_entropy * regularization_loss_entropy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model CNNKAN\n",
        "  - Model CNNKAN disesuaikan untuk ukuran gambar 200x200.\n",
        "\n",
        "  Kode ini menggunakan BCEWithLogisticsLoss karena:\n",
        "    - Stabilitas Numerik: BCEWithLogitsLoss lebih stabil karena menggabungkan sigmoid dan binary cross-entropy dalam satu operasi, mengurangi risiko gradien yang hilang atau meledak.\n",
        "   - Kemudahan Penggunaan: Tidak perlu menerapkan sigmoid secara manual pada output model sebelum menghitung loss, yang membuat kode lebih sederhana dan mengurangi risiko kesalahan.\n",
        "\n",
        "   Jika ingin menggunakan BCELoss, silakan gunakan kode berikut:\n",
        "\n",
        "  class CNNKAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNKAN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.kan1 = KANLinear(64 * 50 * 50, 256)  # Adjusted for input size 200x200\n",
        "        self.kan2 = KANLinear(256, 1)  # Output layer for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.selu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.selu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.kan1(x)\n",
        "        x = torch.sigmoid(self.kan2(x))  # Sigmoid for binary classification\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "i-7-okGsj7Gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNKAN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNKAN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.kan1 = KANLinear(64 * 50 * 50, 256)\n",
        "        self.kan2 = KANLinear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.selu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.selu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.kan1(x)\n",
        "        x = self.kan2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "lQyCl4lxzFP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Menggunakan ImageDataGenerator untuk Dataset Dandelion dan Grass\n",
        "  - Dataset ini disiapkan menggunakan ImageDataGenerator dari Keras dan kemudian dikonversi menjadi PyTorch Dataset."
      ],
      "metadata": {
        "id": "jT75DnnNkXw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Directory paths\n",
        "train_dir = '/content/drive/MyDrive/000-dataset-catdog-25000/train'\n",
        "val_dir = '/content/drive/MyDrive/000-dataset-catdog-25000/valid'\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
        "\n",
        "# Create the data loaders with smaller batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "qLSypfsjkQ2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training dan Evaluasi Model\n",
        "  - Bagian ini mendefinisikan loop training dan evaluasi model. Clear Gradients dan Kosongkan Cache"
      ],
      "metadata": {
        "id": "I5SRWBDRkhPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = CNNKAN().to(device)\n",
        "print(model)\n",
        "summary(model, input_size=(3, 200, 200))\n",
        "\n",
        "# Print model parameter details\n",
        "def print_parameter_details(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.size()} {'requires_grad' if param.requires_grad else 'frozen'}\")\n",
        "\n",
        "print_parameter_details(model)\n",
        "\n",
        "# Visualize KAN parameters\n",
        "def visualize_kan_parameters(kan_layer, layer_name):\n",
        "    base_weights = kan_layer.base_weight.data.cpu().numpy()\n",
        "    plt.hist(base_weights.ravel(), bins=50)\n",
        "    plt.title(f\"Distribution of Base Weights - {layer_name}\")\n",
        "    plt.xlabel(\"Weight Value\")\n",
        "    plt.ylabel(\"Frequency\")\n",
        "    plt.show()\n",
        "    if hasattr(kan_layer, 'spline_weight'):\n",
        "        spline_weights = kan_layer.spline_weight.data.cpu().numpy()\n",
        "        plt.hist(spline_weights.ravel(), bins=50)\n",
        "        plt.title(f\"Distribution of Spline Weights - {layer_name}\")\n",
        "        plt.xlabel(\"Weight Value\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.show()\n",
        "\n",
        "# criterion = nn.BCELoss()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
        "\n",
        "# Training function\n",
        "def train(model, train_loader, criterion, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(inputs)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "        # Clear CUDA cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "# Validation function\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
        "            val_loss += loss.item()\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.squeeze() == labels).sum().item()\n",
        "\n",
        "            # Clear CUDA cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return val_loss / len(val_loader), accuracy\n",
        "\n",
        "# Input jumlah epoch\n",
        "jumlah_epoch = int(input(\"Masukkan jumlah epoch: \"))\n",
        "for epoch in range(jumlah_epoch):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device, epoch)\n",
        "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}/{jumlah_epoch}, Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "# Save model weights\n",
        "torch.save(model.state_dict(), 'model_weights_KAN.pth')\n",
        "\n",
        "# Evaluate model on validation set\n",
        "def test(model, val_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.squeeze() == labels).sum().item()\n",
        "\n",
        "            # Clear CUDA cache\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "accuracy = test(model, val_loader, device)\n",
        "print(f'Validation Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x07NBEkvkd5f",
        "outputId": "4d78352a-f929-4b49-a494-5bbbf1059a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNKAN(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (kan1): KANLinear(\n",
            "    (base_activation): SiLU()\n",
            "  )\n",
            "  (kan2): KANLinear(\n",
            "    (base_activation): SiLU()\n",
            "  )\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 200, 200]             896\n",
            "         MaxPool2d-2         [-1, 32, 100, 100]               0\n",
            "            Conv2d-3         [-1, 64, 100, 100]          18,496\n",
            "         MaxPool2d-4           [-1, 64, 50, 50]               0\n",
            "              SiLU-5               [-1, 160000]               0\n",
            "         KANLinear-6                  [-1, 256]               0\n",
            "              SiLU-7                  [-1, 256]               0\n",
            "         KANLinear-8                    [-1, 1]               0\n",
            "================================================================\n",
            "Total params: 19,392\n",
            "Trainable params: 19,392\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.46\n",
            "Forward/backward pass size (MB): 19.54\n",
            "Params size (MB): 0.07\n",
            "Estimated Total Size (MB): 20.07\n",
            "----------------------------------------------------------------\n",
            "conv1.weight: torch.Size([32, 3, 3, 3]) requires_grad\n",
            "conv1.bias: torch.Size([32]) requires_grad\n",
            "conv2.weight: torch.Size([64, 32, 3, 3]) requires_grad\n",
            "conv2.bias: torch.Size([64]) requires_grad\n",
            "kan1.base_weight: torch.Size([256, 160000]) requires_grad\n",
            "kan1.spline_weight: torch.Size([256, 160000, 8]) requires_grad\n",
            "kan1.spline_scaler: torch.Size([256, 160000]) requires_grad\n",
            "kan2.base_weight: torch.Size([1, 256]) requires_grad\n",
            "kan2.spline_weight: torch.Size([1, 256, 8]) requires_grad\n",
            "kan2.spline_scaler: torch.Size([1, 256]) requires_grad\n",
            "Train Epoch: 0 [0/19999 (0%)]\tLoss: 0.692598\n",
            "Train Epoch: 0 [640/19999 (3%)]\tLoss: 0.692866\n",
            "Train Epoch: 0 [1280/19999 (6%)]\tLoss: 0.678678\n",
            "Train Epoch: 0 [1920/19999 (10%)]\tLoss: 0.662750\n",
            "Train Epoch: 0 [2560/19999 (13%)]\tLoss: 0.694609\n",
            "Train Epoch: 0 [3200/19999 (16%)]\tLoss: 0.665663\n",
            "Train Epoch: 0 [3840/19999 (19%)]\tLoss: 0.682646\n",
            "Train Epoch: 0 [4480/19999 (22%)]\tLoss: 0.672922\n",
            "Train Epoch: 0 [5120/19999 (26%)]\tLoss: 0.656800\n",
            "Train Epoch: 0 [5760/19999 (29%)]\tLoss: 0.682813\n",
            "Train Epoch: 0 [6400/19999 (32%)]\tLoss: 0.657237\n",
            "Train Epoch: 0 [7040/19999 (35%)]\tLoss: 0.637309\n",
            "Train Epoch: 0 [7680/19999 (38%)]\tLoss: 0.664688\n",
            "Train Epoch: 0 [8320/19999 (42%)]\tLoss: 0.533066\n",
            "Train Epoch: 0 [8960/19999 (45%)]\tLoss: 0.595783\n",
            "Train Epoch: 0 [9600/19999 (48%)]\tLoss: 0.672516\n",
            "Train Epoch: 0 [10240/19999 (51%)]\tLoss: 0.647757\n",
            "Train Epoch: 0 [10880/19999 (54%)]\tLoss: 0.658469\n",
            "Train Epoch: 0 [11520/19999 (58%)]\tLoss: 0.740355\n",
            "Train Epoch: 0 [12160/19999 (61%)]\tLoss: 0.622667\n",
            "Train Epoch: 0 [12800/19999 (64%)]\tLoss: 0.648655\n",
            "Train Epoch: 0 [13440/19999 (67%)]\tLoss: 0.615644\n",
            "Train Epoch: 0 [14080/19999 (70%)]\tLoss: 0.682940\n",
            "Train Epoch: 0 [14720/19999 (73%)]\tLoss: 0.555529\n",
            "Train Epoch: 0 [15360/19999 (77%)]\tLoss: 0.614754\n",
            "Train Epoch: 0 [16000/19999 (80%)]\tLoss: 0.608627\n",
            "Train Epoch: 0 [16640/19999 (83%)]\tLoss: 0.728752\n",
            "Train Epoch: 0 [17280/19999 (86%)]\tLoss: 0.644323\n",
            "Train Epoch: 0 [17920/19999 (89%)]\tLoss: 0.554062\n",
            "Train Epoch: 0 [18560/19999 (93%)]\tLoss: 0.650914\n",
            "Train Epoch: 0 [19200/19999 (96%)]\tLoss: 0.531545\n",
            "Train Epoch: 0 [19840/19999 (99%)]\tLoss: 0.585632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluasi Model pada Data Test"
      ],
      "metadata": {
        "id": "eKxSJ3kalFhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Create the test dataset and loader\n",
        "test_dir = '/tmp/test'\n",
        "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            predicted = (outputs > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted.squeeze() == labels).sum().item()\n",
        "\n",
        "            # Kosongkan cache CUDA\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "accuracy = test(model, test_loader, device)\n",
        "print(f'Test Accuracy: {accuracy}%')\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Ps6sCTCjlHRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot ROC Curve"
      ],
      "metadata": {
        "id": "0uUQZTx6jSQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "\n",
        "# Function to calculate predictions and true labels\n",
        "def get_predictions_and_labels(model, val_loader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(preds), np.array(true_labels)\n",
        "\n",
        "# Function to plot ROC Curve\n",
        "def plot_roc_curve(true_labels, preds):\n",
        "    fpr, tpr, _ = roc_curve(true_labels, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic with KAN-CNN')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Function to calculate and print classification metrics\n",
        "def print_classification_metrics(true_labels, preds):\n",
        "    preds_binary = (preds >= 0.5).astype(int) # Threshold for binary classification\n",
        "    accuracy = accuracy_score(true_labels, preds_binary)\n",
        "    precision = precision_score(true_labels, preds_binary)\n",
        "    recall = recall_score(true_labels, preds_binary)\n",
        "    f1 = f1_score(true_labels, preds_binary)\n",
        "    report = classification_report(true_labels, preds_binary, target_names=['Class 0', 'Class 1'])\n",
        "\n",
        "    print(\" \")\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "    print('Classification Report:')\n",
        "    print(report)\n",
        "\n",
        "# Assuming you have your model, val_loader, and device already defined\n",
        "# Example usage\n",
        "preds, true_labels = get_predictions_and_labels(model, val_loader, device)\n",
        "plot_roc_curve(true_labels, preds)\n",
        "print_classification_metrics(true_labels, preds)\n"
      ],
      "metadata": {
        "id": "-w4FkFyOi8zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Prediction"
      ],
      "metadata": {
        "id": "NUCdX5XfjVj2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSmDFZMBAb88"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((200, 200)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Upload images\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Predict and display images\n",
        "for fn in uploaded.keys():\n",
        "    # Load and preprocess image\n",
        "    path = fn\n",
        "    img = Image.open(path).convert('RGB')\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(img_tensor)\n",
        "        prob = torch.sigmoid(output).item()\n",
        "\n",
        "    # Display image\n",
        "    plt.imshow(np.array(img))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    # Print prediction\n",
        "    print(f\"Prediksi untuk {fn}: {prob:.4f}\")\n",
        "    if prob < 0.5:\n",
        "        print(f\"{fn} ini adalah bunga dandelion\")\n",
        "    else:\n",
        "        print(f\"{fn} ini adalah rumput\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "Run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "651IgjLyo-Jx"
      },
      "outputs": [],
      "source": [
        "# import os, signal\n",
        "# os.kill(os.getpid(), signal.SIGKILL)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}