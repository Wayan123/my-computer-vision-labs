{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.optim.lr_scheduler import OneCycleLR"
      ],
      "metadata": {
        "id": "91v6XzwUYI4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the KANLinear class as provided\n",
        "class KANLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, grid_size=10, spline_order=3, scale_noise=0.05, scale_base=0.5, scale_spline=0.5, enable_standalone_scale_spline=True, base_activation=nn.SiLU, grid_eps=0.02, grid_range=[-1, 1]):\n",
        "        super(KANLinear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.grid_size = grid_size\n",
        "        self.spline_order = spline_order\n",
        "\n",
        "        h = (grid_range[1] - grid_range[0]) / grid_size\n",
        "        grid = ((torch.arange(-spline_order, grid_size + spline_order + 1) * h + grid_range[0]).expand(in_features, -1).contiguous())\n",
        "        self.register_buffer(\"grid\", grid)\n",
        "\n",
        "        self.base_weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.spline_weight = nn.Parameter(torch.Tensor(out_features, in_features, grid_size + spline_order))\n",
        "        if enable_standalone_scale_spline:\n",
        "            self.spline_scaler = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "\n",
        "        self.scale_noise = scale_noise\n",
        "        self.scale_base = scale_base\n",
        "        self.scale_spline = scale_spline\n",
        "        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n",
        "        self.base_activation = base_activation()\n",
        "        self.grid_eps = grid_eps\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n",
        "        with torch.no_grad():\n",
        "            noise = ((torch.rand(self.grid_size + 1, self.in_features, self.out_features) - 1 / 2) * self.scale_noise / self.grid_size)\n",
        "            self.spline_weight.data.copy_((self.scale_spline if not self.enable_standalone_scale_spline else 1.0) * self.curve2coeff(self.grid.T[self.spline_order : -self.spline_order], noise))\n",
        "            if self.enable_standalone_scale_spline:\n",
        "                nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n",
        "\n",
        "    def b_splines(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        grid = self.grid\n",
        "        x = x.unsqueeze(-1)\n",
        "        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n",
        "        for k in range(1, self.spline_order + 1):\n",
        "            bases = ((x - grid[:, : -(k + 1)]) / (grid[:, k:-1] - grid[:, : -(k + 1)]) * bases[:, :, :-1]) + ((grid[:, k + 1 :] - x) / (grid[:, k + 1 :] - grid[:, 1:(-k)]) * bases[:, :, 1:])\n",
        "        assert bases.size() == (x.size(0), self.in_features, self.grid_size + self.spline_order)\n",
        "        return bases.contiguous()\n",
        "\n",
        "    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        assert y.size() == (x.size(0), self.in_features, self.out_features)\n",
        "        A = self.b_splines(x).transpose(0, 1)\n",
        "        B = y.transpose(0, 1)\n",
        "        solution = torch.linalg.lstsq(A, B).solution\n",
        "        result = solution.permute(2, 0, 1)\n",
        "        assert result.size() == (self.out_features, self.in_features, self.grid_size + self.spline_order)\n",
        "        return result.contiguous()\n",
        "\n",
        "    @property\n",
        "    def scaled_spline_weight(self):\n",
        "        return self.spline_weight * (self.spline_scaler.unsqueeze(-1) if self.enable_standalone_scale_spline else 1.0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        base_output = F.linear(self.base_activation(x), self.base_weight)\n",
        "        spline_output = F.linear(self.b_splines(x).view(x.size(0), -1), self.scaled_spline_weight.view(self.out_features, -1))\n",
        "        return base_output + spline_output\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_grid(self, x: torch.Tensor, margin=0.01):\n",
        "        assert x.dim() == 2 and x.size(1) == self.in_features\n",
        "        batch = x.size(0)\n",
        "        splines = self.b_splines(x).permute(1, 0, 2)\n",
        "        orig_coeff = self.scaled_spline_weight.permute(1, 2, 0)\n",
        "        unreduced_spline_output = torch.bmm(splines, orig_coeff).permute(1, 0, 2)\n",
        "        x_sorted = torch.sort(x, dim=0)[0]\n",
        "        grid_adaptive = x_sorted[torch.linspace(0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device)]\n",
        "        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n",
        "        grid_uniform = (torch.arange(self.grid_size + 1, dtype=torch.float32, device=x.device).unsqueeze(1) * uniform_step + x_sorted[0] - margin)\n",
        "        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n",
        "        grid = torch.cat([grid[:1] - uniform_step * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1), grid, grid[-1:] + uniform_step * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1)], dim=0)\n",
        "        self.grid.copy_(grid.T)\n",
        "        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n",
        "\n",
        "    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n",
        "        l1_fake = self.spline_weight.abs().mean(-1)\n",
        "        regularization_loss_activation = l1_fake.sum()\n",
        "        p = l1_fake / regularization_loss_activation\n",
        "        regularization_loss_entropy = -torch.sum(p * p.log())\n",
        "        return regularize_activation * regularization_loss_activation + regularize_entropy * regularization_loss_entropy"
      ],
      "metadata": {
        "id": "n3Ryr9848wL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture with CustomResNet using KANLinear\n",
        "class CustomResNetKAN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(CustomResNetKAN, self).__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "        self.model.fc = KANLinear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Define data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize model, loss function, optimizer, and scheduler\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CustomResNetKAN().to(device)\n",
        "print(model)\n",
        "\n",
        "# Print model parameter details\n",
        "def print_parameter_details(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}: {param.size()} {'requires_grad' if param.requires_grad else 'frozen'}\")\n",
        "\n",
        "print_parameter_details(model)\n",
        "\n",
        "summary(model, input_size=(3, 32, 32))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(train_loader), epochs=200)\n",
        "\n",
        "# Training and validation functions\n",
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "    return running_loss / len(train_loader.dataset)\n",
        "\n",
        "def validate(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    return val_loss / len(test_loader.dataset), 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "patience = 10\n",
        "patience_counter = 0\n",
        "\n",
        "# Lists to store loss and accuracy values\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(200):\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_accuracy = validate(model, test_loader, criterion, device)\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save losses and accuracy\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/200, Train Loss: {train_loss:.6f}, Validation Loss: {val_loss:.6f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), 'best_model_resnet50.pth')\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load('best_model_resnet50.pth'))\n",
        "\n",
        "# Evaluate model on test set\n",
        "accuracy = validate(model, test_loader, criterion, device)[1]\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Plot training and validation loss\n",
        "def plot_loss(train_losses, val_losses):\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot validation accuracy\n",
        "def plot_accuracy(val_accuracies):\n",
        "    plt.figure()\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the losses and accuracy\n",
        "plot_loss(train_losses, val_losses)\n",
        "plot_accuracy(val_accuracies)"
      ],
      "metadata": {
        "id": "clmfs3GJXzgU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}